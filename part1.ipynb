{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import openai\n",
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
    "import torch\n",
    "\n",
    "# Load the pre-trained GPT-2 model and tokenizer\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "model = GPT2LMHeadModel.from_pretrained('gpt2')\n",
    "\n",
    "# Load a valued aligned model\n",
    "openai.api_key = \"{OPENAI_API_KEY}\"\n",
    "\n",
    "# TODO: tweak the prompt\n",
    "def get_aligned_response(question, model_name=\"gpt-3.5-turbo\", temperature=0.8):\n",
    "    prompt = f\"\"\"Answer the following question in one paragraph, be concise.\n",
    "    Question: {question}\"\"\"\n",
    "\n",
    "    for i in range(5):  # 5 attempts with exponential backoff\n",
    "        try:\n",
    "            response = openai.ChatCompletion.create(\n",
    "                model=model_name,\n",
    "                messages=[\n",
    "                    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "                    {\"role\": \"user\", \"content\": prompt}\n",
    "                ],\n",
    "                temperature=temperature,\n",
    "            )\n",
    "            return response['choices'][0]['message']['content'].strip()\n",
    "        except openai.error.OpenAIError as e:\n",
    "            if i == 4:  # On the last attempt, raise the exception\n",
    "                raise e\n",
    "            else:\n",
    "                time.sleep((2 ** i) + 1)  # Exponential backoff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sage import sage\n",
    "\n",
    "def get_target_response(question: list, model, tokenizer):\n",
    "    pass\n",
    "\n",
    "def evaluate_model(model, tokenizer, questions, is_target=False):\n",
    "    # some other handling\n",
    "    if is_target:\n",
    "        response = get_target_response(questions, model, tokenizer)\n",
    "    else:\n",
    "        response = get_aligned_response(questions)\n",
    "    return sage.score(questions, response, use_rots=True)\n",
    "\n",
    "\n",
    "questions = ['What makes us human?']\n",
    "\n",
    "# Evaluate the baseline model\n",
    "baseline_results = evaluate_model(model, tokenizer, questions, is_target=True)\n",
    "print(\"Baseline Evaluation Results:\", baseline_results)\n",
    "\n",
    "# Evaluate the aligned model\n",
    "aligned_results = evaluate_model(None, None, questions, is_target=False)\n",
    "print(\"Aligned Evaluation Results:\", aligned_results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fine-tune the GPT-2 model using LoRA\n",
    "from lora import LoRA\n",
    "\n",
    "# TODO might need to add more layers to the lora model\n",
    "# Initialize LoRA with the GPT-2 model\n",
    "lora = LoRA(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Start the GAN training here\n",
    "training_data = []\n",
    "# create network for target model answer to our questions\n",
    "# aligned model will answer the questions as well\n",
    "# if the aligned model answer scores well on sage, feed it into training data\n",
    "def create_training_data(questions, target_model, tokenizer, threshold):\n",
    "    training_data = []\n",
    "    feedback_loop = []\n",
    "\n",
    "    for question in questions:\n",
    "        target_response = get_target_response(question, target_model, tokenizer)\n",
    "        aligned_response = get_aligned_response(question)\n",
    "        \n",
    "        target_score = sage.score(question, target_response, use_rots=True)\n",
    "        aligned_score = sage.score(question, aligned_response, use_rots=True)\n",
    "        \n",
    "        if aligned_score > threshold:\n",
    "            training_data.append((question, aligned_response))\n",
    "        elif target_score > aligned_score:\n",
    "            feedback_loop.append((question, target_response, aligned_response))\n",
    "    \n",
    "    return training_data, feedback_loop\n",
    "\n",
    "\n",
    "# TODO add more questions to ask\n",
    "# TODO use chatGPT to generate the qn for us\n",
    "questions = ['What makes us human?']\n",
    "threshold = 0.8  # Define a threshold for good responses\n",
    "\n",
    "training_data, feedback_loop = create_training_data(questions, model, tokenizer, threshold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# examine in which cases the target model is better than the aligned model\n",
    "feedback_loop\n",
    "\n",
    "# process questions and add them into the training data\n",
    "# print out the responses for a quick check\n",
    "for i in range(5):\n",
    "    question, target_response, aligned_response = feedback_loop[i]\n",
    "    print(f\"Question: {question}\")\n",
    "    print(f\"Target Response: {target_response}\")\n",
    "    print(f\"Aligned Response: {aligned_response}\")\n",
    "    print()\n",
    "\n",
    "# TODO how should we handle these cases ?\n",
    "\n",
    "# add the feedback loop into the training data\n",
    "training_data.extend([(question, target_response) for question, target_response, _ in feedback_loop])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model with LoRA\n",
    "lora.train(training_data)\n",
    "\n",
    "# Save the fine-tuned model\n",
    "lora.save_pretrained('lora_finetuned_gpt2_a')\n",
    "lora.save_pretrained('lora_finetuned_gpt2_b')"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
